{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ReLU is a popular activation function since it is differentiable and nonlinear. If the inputs are negative its derivative becomes zero which causes the ‘dying’ of neurons and learning doesn’t take place. Let us illustrate the use of ReLU with the help of the Python program"
      ],
      "metadata": {
        "id": "V_bpWYWxOdgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZmUH_iJOWY_",
        "outputId": "fbf48119-4616-4692-ce5e-47e29d68c7d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 0., 3., 0.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# defining relu\n",
        "r = nn.ReLU()\n",
        "\n",
        "# Creating a Tensor with an array\n",
        "input = torch.Tensor([1, -2, 3, -5])\n",
        "\n",
        "# Passing the array to relu function\n",
        "output = r(input)\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leaky ReLU Activation Function:\n",
        "Leaky ReLU Activation Function or LReLU is another type of activation function which is similar to ReLU but solves the problem of ‘dying’ neurons and, graphically Leaky ReLU has the following transformative behavior:\n",
        "\n",
        "Leaky ReLU Activation Function\n",
        "\n",
        "\n",
        "This function is very useful as when the input is negative the differentiation of the function is not zero. Hence the learning of neurons doesn’t stop.  Let us illustrate the use of LReLU with the help of the Python program."
      ],
      "metadata": {
        "id": "BxDA959SOmEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# defining Lrelu and the parameter 0.2 is passed to control the negative slope ; a=0.2\n",
        "r = nn.LeakyReLU(0.2)\n",
        "\n",
        "# Creating a Tensor with an array\n",
        "input = torch.Tensor([1,-2,3,-5])\n",
        "\n",
        "output = r(input)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMlM5Fa3Om-z",
        "outputId": "796c91a9-2c3a-4321-a082-8e10def11fae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 1.0000, -0.4000,  3.0000, -1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Activation Function:\n",
        "Sigmoid Function is a non-linear and differentiable activation function. It is an S-shaped curve that does not pass through the origin. It produces an output that lies between 0 and 1. The output values are often treated as a probability. It is often used for binary classification. It is slow in computation and, graphically Sigmoid has the following transformative behavior:\n",
        "\n",
        "Sigmoid Activation Function\n",
        "\n",
        "\n",
        "Sigmoid activation function has a problem of “Vanishing Gradient”.  Vanishing Gradient is a significant problem as a large number of inputs are fed to the neural network and the number of hidden layers increases, the gradient or derivative becomes close to zero thus leading to inaccuracy in the neural network.\n",
        "\n",
        "Let us illustrate the use of the Sigmoid function with the help of a Python Program."
      ],
      "metadata": {
        "id": "Ohp01xSsOs3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Calling the sigmoid function\n",
        "sig = nn.Sigmoid()\n",
        "\n",
        "# Defining tensor\n",
        "input = torch.Tensor([1,-2,3,-5])\n",
        "\n",
        "# Applying sigmoid to the tensor\n",
        "output = sig(input)\n",
        "\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SO7ouH4OthA",
        "outputId": "e53f9ea0-4b49-4de4-e88d-0c71771c6a62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.7311, 0.1192, 0.9526, 0.0067])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanh Activation Function:\n",
        "Tanh function is a non-linear and differentiable function similar to the sigmoid function but output values range from -1 to +1. It is an S-shaped curve that passes through the origin and, graphically Tanh has the following transformative behavior:\n",
        "\n",
        "Tanh Activation Function\n",
        "\n",
        "\n",
        "The problem with the Tanh Activation function is it is slow and the vanishing gradient problem persists. Let us illustrate the use of the Tanh function with the help of a Python Program.\n",
        "\n"
      ],
      "metadata": {
        "id": "FTK_xsziOxje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Calling the Tanh function\n",
        "t = nn.Tanh()\n",
        "\n",
        "# Defining tensor\n",
        "input = torch.Tensor([1,-2,3,-5])\n",
        "\n",
        "# Applying Tanh to the tensor\n",
        "output = t(input)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UozVrr43OzWb",
        "outputId": "275c469c-f0c8-4b8e-aaf1-80b9a92ae577"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.7616, -0.9640,  0.9951, -0.9999])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Activation Function:\n",
        "The softmax function is different from other activation functions as it is placed at the last to normalize the output. We can use other activation functions in combination with Softmax to produce the output in probabilistic form. It is used in multiclass classification and generates an output of probabilities whose sum is 1. The range of output lies between 0 and 1. Softmax has the following transformative behavior:\n",
        "\n",
        "Softmax Activation Function\n",
        "\n",
        "\n",
        "Let us illustrate with the help of the Python Program:\n",
        "\n"
      ],
      "metadata": {
        "id": "MGEDutfXO2TH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Calling the Softmax function with\n",
        "# dimension = 0 as dimension starts\n",
        "# from 0\n",
        "sm = nn.Softmax(dim=0)\n",
        "\n",
        "# Defining tensor\n",
        "input = torch.Tensor([1,-2,3,-5])\n",
        "\n",
        "# Applying function to the tensor\n",
        "output = sm(input)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfQ-3TfRO4H-",
        "outputId": "eac51f41-9edb-4c2d-82d3-4695fdb67571"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.1846e-01, 5.8980e-03, 8.7534e-01, 2.9365e-04])\n"
          ]
        }
      ]
    }
  ]
}